# Dilu
Dilu is a GPU-based serverless deep-learning system that realizes **Introspective Elasticity (IE)** — a fine-grained, two-dimensional co-scaling mechanism enabling GPU resourcing-on-demand.

> This repository contains a simplified reference implementation
> for the paper  
> **“Dilu: Enabling GPU Resourcing-on-Demand for Serverless DL Serving via Introspective Elasticity.”**

## Features
* **Multi-factor profiling** with efficient pruning search  
* **Resourcing-complementary scheduling** for high GPU utilization under QoS constraints  
* **Adaptive 2D co-scaling** (vertical & horizontal) with real-time decisions

## Requirements
* PyTorch 1.11
* DeepSpeed 0.11.1
* NCCL 2.10.3
* CUDA 11.7 + NVIDIA Driver 515.105.01
* Docker 24.0.5

## Usage
### Prerequisites
1. Install Docker
2. Install CUDA and NVIDIA Driver
3. Pull the basic images, such as `lvcunchi1999/torch110cu111_ddp:cluster`, `lvcunchi1999/torch110cu111_deepspeed:latest`

### Build Images and Run
See the README.md files of each subfolder for more details. The main steps are as follows:

0. Profiling to get the resource configurations.
1. Start the RCKM server on each node (see adaptive_2D_scaling/vertical_scaling/README.md).
2. Start the scaler and scheduler (see cluster_scheduling/README.md).
3. Deploy train/inference tasks.
4. Generate inference workloads.

## Citation

```bibtex
@inproceedings{lv2025dilu,
  title={Dilu: Enabling GPU Resourcing-on-Demand for Serverless DL Serving via Introspective Elasticity},
  author={Lv, Cunchi and Shi, Xiao and Lei, Zhengyu and Huang, Jinyue and Tan, Wenting and Zheng, Xiaohui and Zhao, Xiaofang},
  booktitle={Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
  pages={311--325},
  year={2025}
}
```
